{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Honeypot? Event Streaming for The Rest of Us Honeypot is a multi-protocol event collection, validation, routing, and observability system. It is designed to be easily-configured , easily-deployed , and easily-maintained . Yet uncompromising with its speed, guarantees, and operational flexibility. Quickstart To dive head-first into an example of running Honeypot locally with a three-node Redpanda cluster, Kowl , and Materialize please see the Quickstart . Philosophy Build new systems on proven API's and mental models There are some very good API's out there. Oftentimes these API's were originally built using best-in-class tech, but said tech has since been supplanted. Why recreate the world when advancing an existing API or mental model will do the trick? Examples of building new technology on top of pre-existing API's include: Redpanda , which is built on Kafka 's API. Materialize , whose clients are built on the Postgres API. Airbyte , which builds upon the conceptual model of Fivetran or Stitch (and the technical model of Singer ). Timescale , which builds on the Postgres API. PipelineDB , which was built on the Postgres API. Stand on the shoulders of giants Data systems and infrastructure are getting very very cool . When popular event tracking systems like Snowplow Analytics were first created, deployment systems like K8S and Knative didn't exist. Nor did the \"serverless\" mindset. Kafka was an infant. And had not yet created the things that Redpanda solves. Streaming databases built on the Postgres API were years into the future. And a data warehouse that would eat the world with its developer-focus yet massively-scalable architecture was yet to be named. These are all present-day realities and we want to build upon them with an eye to the future. Scale to zero, but also \"infinity\" Why pay for what you don't use? Or completely rearchitect systems as volume grows or demands change? Serverless scales to zero, and then back up again... Snowflake scales to zero, and then back up again... Event collection systems should too. Validate and redirect on the edge Data should be validated and redirected as soon as it enters collection infrastructure, not near the end of the process (or in the data warehouse). The faster data is declared \"valid\" or \"invalid\", the faster it can be used and acted upon. Keep operational complexity low And last but certainly not least, engineers should be able to maintain and advance event collection efforts without complexity or cost explosion. Would you like to know more? If you would like to know more or follow the project, check out the roadmap or sign up for Insiders-Only Updates .","title":"What is Honeypot?"},{"location":"#what-is-honeypot","text":"","title":"What is Honeypot?"},{"location":"#event-streaming-for-the-rest-of-us","text":"Honeypot is a multi-protocol event collection, validation, routing, and observability system. It is designed to be easily-configured , easily-deployed , and easily-maintained . Yet uncompromising with its speed, guarantees, and operational flexibility.","title":"Event Streaming for The Rest of Us"},{"location":"#quickstart","text":"To dive head-first into an example of running Honeypot locally with a three-node Redpanda cluster, Kowl , and Materialize please see the Quickstart .","title":"Quickstart"},{"location":"#philosophy","text":"","title":"Philosophy"},{"location":"#build-new-systems-on-proven-apis-and-mental-models","text":"There are some very good API's out there. Oftentimes these API's were originally built using best-in-class tech, but said tech has since been supplanted. Why recreate the world when advancing an existing API or mental model will do the trick? Examples of building new technology on top of pre-existing API's include: Redpanda , which is built on Kafka 's API. Materialize , whose clients are built on the Postgres API. Airbyte , which builds upon the conceptual model of Fivetran or Stitch (and the technical model of Singer ). Timescale , which builds on the Postgres API. PipelineDB , which was built on the Postgres API.","title":"Build new systems on proven API's and mental models"},{"location":"#stand-on-the-shoulders-of-giants","text":"Data systems and infrastructure are getting very very cool . When popular event tracking systems like Snowplow Analytics were first created, deployment systems like K8S and Knative didn't exist. Nor did the \"serverless\" mindset. Kafka was an infant. And had not yet created the things that Redpanda solves. Streaming databases built on the Postgres API were years into the future. And a data warehouse that would eat the world with its developer-focus yet massively-scalable architecture was yet to be named. These are all present-day realities and we want to build upon them with an eye to the future.","title":"Stand on the shoulders of giants"},{"location":"#scale-to-zero-but-also-infinity","text":"Why pay for what you don't use? Or completely rearchitect systems as volume grows or demands change? Serverless scales to zero, and then back up again... Snowflake scales to zero, and then back up again... Event collection systems should too.","title":"Scale to zero, but also \"infinity\""},{"location":"#validate-and-redirect-on-the-edge","text":"Data should be validated and redirected as soon as it enters collection infrastructure, not near the end of the process (or in the data warehouse). The faster data is declared \"valid\" or \"invalid\", the faster it can be used and acted upon.","title":"Validate and redirect on the edge"},{"location":"#keep-operational-complexity-low","text":"And last but certainly not least, engineers should be able to maintain and advance event collection efforts without complexity or cost explosion.","title":"Keep operational complexity low"},{"location":"#would-you-like-to-know-more","text":"If you would like to know more or follow the project, check out the roadmap or sign up for Insiders-Only Updates .","title":"Would you like to know more?"},{"location":"insiders-only-updates/","text":"Insiders-Only Updates Do you want to contribute to the project? Are you interested in being the first to know about new developments? Want to know when Honeypot is fully production-ready? Would you like to invest in or advise the project? Please fill out the form below. We'd love to hear from you. hbspt.forms.create({ region: \"na1\", portalId: \"21941653\", formId: \"6f97f0b5-3954-4cb3-b0bf-0928d4433b6d\" });","title":"Insiders-Only Updates"},{"location":"insiders-only-updates/#insiders-only-updates","text":"","title":"Insiders-Only Updates"},{"location":"insiders-only-updates/#do-you-want-to-contribute-to-the-project","text":"","title":"Do you want to contribute to the project?"},{"location":"insiders-only-updates/#are-you-interested-in-being-the-first-to-know-about-new-developments","text":"","title":"Are you interested in being the first to know about new developments?"},{"location":"insiders-only-updates/#want-to-know-when-honeypot-is-fully-production-ready","text":"","title":"Want to know when Honeypot is fully production-ready?"},{"location":"insiders-only-updates/#would-you-like-to-invest-in-or-advise-the-project","text":"","title":"Would you like to invest in or advise the project?"},{"location":"insiders-only-updates/#please-fill-out-the-form-below-wed-love-to-hear-from-you","text":"hbspt.forms.create({ region: \"na1\", portalId: \"21941653\", formId: \"6f97f0b5-3954-4cb3-b0bf-0928d4433b6d\" });","title":"Please fill out the form below. We'd love to hear from you."},{"location":"why/","text":"Why Honeypot? Simplified collection and transport architecture Event collection infrastructure comes in all shapes and sizes. These systems are often burdensome to maintain and operate due to dozens and dozens of moving pieces. Or the lack of important moving pieces that introduce potential data loss. Honeypot was created from the ground-up to eliminate as many moving pieces as possible without sacrificing quality and transport guarantees. TL;DR: Less infrastructure = reduced maintenance and headaches for humans. Less infrastructure = reduced infrastructure costs, increased operational efficiencies. Multi-protocol Event collection systems are often single-protocol -> think separate systems for collecting/pipelining arbitrary webhooks, Snowplow Analytics, Segment, Cloudevents, etc. Honeypot supports a number of common input protocols and will continue to support more. TL;DR: A single, flexible system instead of N pipelines to support N protocols. Efficiences of scale. Event Validation on the Edge Streaming data is all about one thing: increasing the speed of action and decision-making. If events are not validated fast, decisions and actions cannot be made with the conviction they require. Who cares about making bad decisions, on bad data, fast? Nobody. TL;DR: Honeypot validates incoming events the millisecond they are collected. Honeypot empowers faster decision-making and action. Multi-cloud Honeypot ships with both Go binaries and a Docker image. This results in minimal artifacts to deploy, maintain, and monitor, and an entirely cloud-agnostic deployment model. Want to deploy on GCP Cloud Run? \u2705 Want to deploy with GKE, EKS, or in k8s? \u2705 Want to deploy on bare metal? \u2705 TL;DR: Want to have x-cloud event tracking? \u2705 Want to migrate clouds? \u2705 Rich metadata and stream introspection After years of building, maintaining, and managing event tracking systems there's one thing that has always stuck out: more metadata and expedited knowledge of what is happening within the stream would be unbelievably empowering . This usually happens at the tail end of the pipeline - in the data warehouse. Not awesome. TL;DR: Honeypot gives more visibility into events, faster. Flexible destinations Honeypot was purpose-built to give the practitioner ultimate flexibility. Most event collection systems have a single \"destination\", or require setting up more infrastructure to fan out events to multiple places. We see this as wasteful. Multiple Honeypot sinks can be configured regardless of what cloud the destination actually resides in. TL;DR: Honeypot allows the operator to send events to one place. Or five. No additional infrastructure necessary. Easy configuration Honeypot is easily configured with a single yml file. This file is self-validating and, when using the vcode yaml plugin , practically writes itself. Built to scale Honeypot is written entirely in Go which results in a very compact, very useful binary that naturally lends itself to scale. It is written to h-scale quickly and easily without unintended side effects.","title":"Why Honeypot?"},{"location":"why/#why-honeypot","text":"","title":"Why Honeypot?"},{"location":"why/#simplified-collection-and-transport-architecture","text":"Event collection infrastructure comes in all shapes and sizes. These systems are often burdensome to maintain and operate due to dozens and dozens of moving pieces. Or the lack of important moving pieces that introduce potential data loss. Honeypot was created from the ground-up to eliminate as many moving pieces as possible without sacrificing quality and transport guarantees. TL;DR: Less infrastructure = reduced maintenance and headaches for humans. Less infrastructure = reduced infrastructure costs, increased operational efficiencies.","title":"Simplified collection and transport architecture"},{"location":"why/#multi-protocol","text":"Event collection systems are often single-protocol -> think separate systems for collecting/pipelining arbitrary webhooks, Snowplow Analytics, Segment, Cloudevents, etc. Honeypot supports a number of common input protocols and will continue to support more. TL;DR: A single, flexible system instead of N pipelines to support N protocols. Efficiences of scale.","title":"Multi-protocol"},{"location":"why/#event-validation-on-the-edge","text":"Streaming data is all about one thing: increasing the speed of action and decision-making. If events are not validated fast, decisions and actions cannot be made with the conviction they require. Who cares about making bad decisions, on bad data, fast? Nobody. TL;DR: Honeypot validates incoming events the millisecond they are collected. Honeypot empowers faster decision-making and action.","title":"Event Validation on the Edge"},{"location":"why/#multi-cloud","text":"Honeypot ships with both Go binaries and a Docker image. This results in minimal artifacts to deploy, maintain, and monitor, and an entirely cloud-agnostic deployment model. Want to deploy on GCP Cloud Run? \u2705 Want to deploy with GKE, EKS, or in k8s? \u2705 Want to deploy on bare metal? \u2705 TL;DR: Want to have x-cloud event tracking? \u2705 Want to migrate clouds? \u2705","title":"Multi-cloud"},{"location":"why/#rich-metadata-and-stream-introspection","text":"After years of building, maintaining, and managing event tracking systems there's one thing that has always stuck out: more metadata and expedited knowledge of what is happening within the stream would be unbelievably empowering . This usually happens at the tail end of the pipeline - in the data warehouse. Not awesome. TL;DR: Honeypot gives more visibility into events, faster.","title":"Rich metadata and stream introspection"},{"location":"why/#flexible-destinations","text":"Honeypot was purpose-built to give the practitioner ultimate flexibility. Most event collection systems have a single \"destination\", or require setting up more infrastructure to fan out events to multiple places. We see this as wasteful. Multiple Honeypot sinks can be configured regardless of what cloud the destination actually resides in. TL;DR: Honeypot allows the operator to send events to one place. Or five. No additional infrastructure necessary.","title":"Flexible destinations"},{"location":"why/#easy-configuration","text":"Honeypot is easily configured with a single yml file. This file is self-validating and, when using the vcode yaml plugin , practically writes itself.","title":"Easy configuration"},{"location":"why/#built-to-scale","text":"Honeypot is written entirely in Go which results in a very compact, very useful binary that naturally lends itself to scale. It is written to h-scale quickly and easily without unintended side effects.","title":"Built to scale"},{"location":"configuration/sample/","tags":["configuration","sample"],"text":"Sample Configuration version: 1.1 app: env: development mode: debug port: 8080 trackerDomain: trck.slvrtnio.com health: enabled: true path: /health stats: enabled: true path: /stats middleware: timeout: enabled: false ms: 2000 rateLimiter: enabled: false period: S limit: 10 cookie: enabled: true name: nuid secure: false ttlDays: 365 domain: localhost path: / sameSite: Lax cors: enabled: true allowOrigin: - \"*\" allowCredentials: true allowMethods: - POST - OPTIONS - GET maxAge: 86400 requestLogger: enabled: true yeet: enabled: false inputs: snowplow: enabled: true standardRoutesEnabled: true openRedirectsEnabled: true getPath: /plw/g postPath: /plw/p redirectPath: /plw/r anonymize: ip: false userId: false cloudevents: enabled: true path: /ce/p generic: enabled: true path: /gen/p contexts: rootKey: contexts schemaKey: schema dataKey: data payload: rootKey: payload schemaKey: schema dataKey: data webhook: enabled: true path: /wb/hk relay: enabled: true path: /relay schemaCache: backend: type: fs path: ./schemas/ ttlSeconds: 300 maxSizeBytes: 104857600 # 100mb -> 100 * 1024 * 1024 purge: enabled: true path: /c/purge schemaDirectory: enabled: true sinks: - name: nada type: blackhole deliveryRequired: false - name: redpanda type: kafka deliveryRequired: true kafkaBrokers: - 127.0.0.1:9092 validTopic: honeypot-valid invalidTopic: honeypot-invalid - name: mysql type: mysql deliveryRequired: true mysqlHost: 127.0.0.1 mysqlPort: 3306 mysqlDbName: honeypot mysqlUser: honeypot mysqlPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid - name: postgres type: postgres deliveryRequired: true pgHost: 127.0.0.1 pgPort: 5432 pgDbName: honeypot pgUser: honeypot pgPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid - name: materialize type: materialize deliveryRequired: false mzHost: 127.0.0.1 mzPort: 6875 mzDbName: materialize mzUser: materialize mzPass: \"\" validTable: honeypot_valid invalidTable: honeypot_invalid - name: clickhouse type: clickhouse deliveryRequired: true clickhouseHost: 127.0.0.1 clickhousePort: 9000 clickhouseDbName: honeypot clickhouseUser: honeypot clickhousePass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid squawkBox: enabled: true cloudeventsPath: /sqwk/ce snowplowPath: /sqwk/sp genericPath: /sqwk/gen tele: enabled: true","title":"Sample Configuration"},{"location":"configuration/sample/#sample-configuration","text":"version: 1.1 app: env: development mode: debug port: 8080 trackerDomain: trck.slvrtnio.com health: enabled: true path: /health stats: enabled: true path: /stats middleware: timeout: enabled: false ms: 2000 rateLimiter: enabled: false period: S limit: 10 cookie: enabled: true name: nuid secure: false ttlDays: 365 domain: localhost path: / sameSite: Lax cors: enabled: true allowOrigin: - \"*\" allowCredentials: true allowMethods: - POST - OPTIONS - GET maxAge: 86400 requestLogger: enabled: true yeet: enabled: false inputs: snowplow: enabled: true standardRoutesEnabled: true openRedirectsEnabled: true getPath: /plw/g postPath: /plw/p redirectPath: /plw/r anonymize: ip: false userId: false cloudevents: enabled: true path: /ce/p generic: enabled: true path: /gen/p contexts: rootKey: contexts schemaKey: schema dataKey: data payload: rootKey: payload schemaKey: schema dataKey: data webhook: enabled: true path: /wb/hk relay: enabled: true path: /relay schemaCache: backend: type: fs path: ./schemas/ ttlSeconds: 300 maxSizeBytes: 104857600 # 100mb -> 100 * 1024 * 1024 purge: enabled: true path: /c/purge schemaDirectory: enabled: true sinks: - name: nada type: blackhole deliveryRequired: false - name: redpanda type: kafka deliveryRequired: true kafkaBrokers: - 127.0.0.1:9092 validTopic: honeypot-valid invalidTopic: honeypot-invalid - name: mysql type: mysql deliveryRequired: true mysqlHost: 127.0.0.1 mysqlPort: 3306 mysqlDbName: honeypot mysqlUser: honeypot mysqlPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid - name: postgres type: postgres deliveryRequired: true pgHost: 127.0.0.1 pgPort: 5432 pgDbName: honeypot pgUser: honeypot pgPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid - name: materialize type: materialize deliveryRequired: false mzHost: 127.0.0.1 mzPort: 6875 mzDbName: materialize mzUser: materialize mzPass: \"\" validTable: honeypot_valid invalidTable: honeypot_invalid - name: clickhouse type: clickhouse deliveryRequired: true clickhouseHost: 127.0.0.1 clickhousePort: 9000 clickhouseDbName: honeypot clickhouseUser: honeypot clickhousePass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid squawkBox: enabled: true cloudeventsPath: /sqwk/ce snowplowPath: /sqwk/sp genericPath: /sqwk/gen tele: enabled: true","title":"Sample Configuration"},{"location":"deployment/aws/","tags":["deployment","aws"],"text":"AWS","title":"AWS"},{"location":"deployment/aws/#aws","text":"","title":"AWS"},{"location":"deployment/azure/","tags":["deployment","azure"],"text":"Azure","title":"Azure"},{"location":"deployment/azure/#azure","text":"","title":"Azure"},{"location":"deployment/gcp/","tags":["deployment","GCP"],"text":"GCP","title":"GCP"},{"location":"deployment/gcp/#gcp","text":"","title":"GCP"},{"location":"examples/redpanda-kowl-materialize/","tags":["examples","redpanda","kowl","materialize"],"text":"Redpanda, Kowl, and Materialize","title":"Redpanda, Kowl, and Materialize"},{"location":"examples/redpanda-kowl-materialize/#redpanda-kowl-and-materialize","text":"","title":"Redpanda, Kowl, and Materialize"},{"location":"inputs/cdc/","tags":["collector","cdc"],"text":"\ud83d\udfe1 Honeypot CDC Honeypot CDC inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 Honeypot CDC"},{"location":"inputs/cdc/#honeypot-cdc","text":"Honeypot CDC inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 Honeypot CDC"},{"location":"inputs/cloudevents/","tags":["collector","input protocol","cloud events"],"text":"\ud83d\udfe2 CloudEvents Collection Method Honeypot listens on a configurable endpoint for incoming POST requests of Cloudevents payloads . This endpoint requires one of the following content types to be designated: application/cloudevents+json (for single cloudevents) application/cloudevents-batch+json (for a batch of cloudevents) Note! If a Content-Type header is not specified, the event will not be accepted. Validation Method Honeypot validates incoming cloudevents using the dataschema property of each event. Note! If the dataschema property is not present in incoming events, these events will be redirected to the invalid destination(s). Sample Cloudevents Configuration cloudevents: enabled: true # Whether or not to enable the Cloudevents collection endpoint path: /ce/p # Path for incoming (single or batch) cloudevents","title":"\ud83d\udfe2 CloudEvents"},{"location":"inputs/cloudevents/#cloudevents","text":"","title":"\ud83d\udfe2 CloudEvents"},{"location":"inputs/cloudevents/#collection-method","text":"Honeypot listens on a configurable endpoint for incoming POST requests of Cloudevents payloads . This endpoint requires one of the following content types to be designated: application/cloudevents+json (for single cloudevents) application/cloudevents-batch+json (for a batch of cloudevents) Note! If a Content-Type header is not specified, the event will not be accepted.","title":"Collection Method"},{"location":"inputs/cloudevents/#validation-method","text":"Honeypot validates incoming cloudevents using the dataschema property of each event. Note! If the dataschema property is not present in incoming events, these events will be redirected to the invalid destination(s).","title":"Validation Method"},{"location":"inputs/cloudevents/#sample-cloudevents-configuration","text":"cloudevents: enabled: true # Whether or not to enable the Cloudevents collection endpoint path: /ce/p # Path for incoming (single or batch) cloudevents","title":"Sample Cloudevents Configuration"},{"location":"inputs/mqtt/","tags":["collector","input protocol","opc ua"],"text":"\ud83d\udfe1 MQTT MQTT inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 MQTT"},{"location":"inputs/mqtt/#mqtt","text":"MQTT inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 MQTT"},{"location":"inputs/opcua/","tags":["collector","input protocol","opc ua"],"text":"\ud83d\udfe1 OPC-UA OPC-UA inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 OPC-UA"},{"location":"inputs/opcua/#opc-ua","text":"OPC-UA inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 OPC-UA"},{"location":"inputs/pixel/","tags":["collector","input protocol","relay"],"text":"\ud83d\udfe2 Pixel Collection Method Parameter Payloads Honeypot supports collecting payloads via url query params. While this method of data collection is useful it does have drawbacks such as max uri lengths and the inability to explicitly declare parameter types. The good thing about the pixel input is it is extremely simple to get started with. For example -> if /pxl is configured as a pixel input, submitting a GET request to /pxl/?hello=world&userId=10 will send a payload of {\"hello\": \"world\", \"userId\": \"10\"} to the configured sinks. No sdk's necessary. Base64 Encoded Parameter Payloads Honeypot supports a \"special\" query param, hbp , by which b64 encoded payloads can be collected. For example -> if /pxl is configured as a pixel input, submitting a GET request to /pxl?hbp=eyJoZWxsbyI6IndvcmxkIn0 will send a payload of {\"hello\":\"world\"} to the configured sinks. Validation Method Honeypot does not yet validate incoming pixel-based payloads - they are assumed \"ok\" and are sunk to the \"good\" destination. Sample Pixel Configuration pixel: enabled: true paths: - name: default path: /pxl/d - name: secondary path: /pxl/scnd","title":"\ud83d\udfe2 Pixel"},{"location":"inputs/pixel/#pixel","text":"","title":"\ud83d\udfe2 Pixel"},{"location":"inputs/pixel/#collection-method","text":"","title":"Collection Method"},{"location":"inputs/pixel/#parameter-payloads","text":"Honeypot supports collecting payloads via url query params. While this method of data collection is useful it does have drawbacks such as max uri lengths and the inability to explicitly declare parameter types. The good thing about the pixel input is it is extremely simple to get started with. For example -> if /pxl is configured as a pixel input, submitting a GET request to /pxl/?hello=world&userId=10 will send a payload of {\"hello\": \"world\", \"userId\": \"10\"} to the configured sinks. No sdk's necessary.","title":"Parameter Payloads"},{"location":"inputs/pixel/#base64-encoded-parameter-payloads","text":"Honeypot supports a \"special\" query param, hbp , by which b64 encoded payloads can be collected. For example -> if /pxl is configured as a pixel input, submitting a GET request to /pxl?hbp=eyJoZWxsbyI6IndvcmxkIn0 will send a payload of {\"hello\":\"world\"} to the configured sinks.","title":"Base64 Encoded Parameter Payloads"},{"location":"inputs/pixel/#validation-method","text":"Honeypot does not yet validate incoming pixel-based payloads - they are assumed \"ok\" and are sunk to the \"good\" destination.","title":"Validation Method"},{"location":"inputs/pixel/#sample-pixel-configuration","text":"pixel: enabled: true paths: - name: default path: /pxl/d - name: secondary path: /pxl/scnd","title":"Sample Pixel Configuration"},{"location":"inputs/relay/","tags":["collector","input protocol","relay"],"text":"\ud83d\udfe2 Honeypot Relay Collection Methods Honeypot is capable of collecting and distributing events relayed from other honeypot instances, which allows for operational flexibility in unique use cases. Validation Method Relayed messages are not re-validated since messages are validated at point of initial collection. Sample Relay Configuration relay: enabled: true # Whether or not to accept relayed events path: /relay # Path for incoming relayed events","title":"\ud83d\udfe2 Honeypot Relay"},{"location":"inputs/relay/#honeypot-relay","text":"","title":"\ud83d\udfe2 Honeypot Relay"},{"location":"inputs/relay/#collection-methods","text":"Honeypot is capable of collecting and distributing events relayed from other honeypot instances, which allows for operational flexibility in unique use cases.","title":"Collection Methods"},{"location":"inputs/relay/#validation-method","text":"Relayed messages are not re-validated since messages are validated at point of initial collection.","title":"Validation Method"},{"location":"inputs/relay/#sample-relay-configuration","text":"relay: enabled: true # Whether or not to accept relayed events path: /relay # Path for incoming relayed events","title":"Sample Relay Configuration"},{"location":"inputs/ros/","tags":["collector","input protocol","ros"],"text":"\ud83d\udfe1 ROS","title":"\ud83d\udfe1 ROS"},{"location":"inputs/ros/#ros","text":"","title":"\ud83d\udfe1 ROS"},{"location":"inputs/segment/","tags":["collector","input protocol","segment"],"text":"\ud83d\udfe1 Segment Segment inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 Segment"},{"location":"inputs/segment/#segment","text":"Segment inputs are not yet supported. Stay tuned \ud83c\udff4\u200d\u2620\ufe0f\ud83d\ude09\ud83d\ude2e.","title":"\ud83d\udfe1 Segment"},{"location":"inputs/self-describing/","tags":["collector","input protocol","self describing"],"text":"\ud83d\udfe2 Self-Describing Events Collection Method Honeypot listens on a configurable endpoint for incoming POST requests of self describing payloads, structured as: { $CONTEXTS_ROOT_KEY: { \"some-context-schema\": {\"context-data\": \"here\"}, \"another-context-schema\": {\"more-context-data\": \"here\"} }, $PAYLOAD_ROOT_KEY: { $PAYLOAD_SCHEMA_KEY: \"some-key\", $PAYLOAD_DATA_KEY: {\"data\": \"here\"} } } This (configured by you!) endpoint accepts batches of self-describing events and single self-describing events . It requires a Content-Type header of application/json . Note! If a Content-Type header is not specified, the event will not be accepted. Validation Method Honeypot uses the schema defined at $PAYLOAD_KEY.$SCHEMA_KEY to validate each payload. Sample Self-Describing Event Configuration generic: enabled: true # Whether or not to enable generic self-describing events path: /gen/p # Path for incoming self-describing events contexts: rootKey: contexts # The contexts root key (contexts) payload: rootKey: payload # The payload root key (payload) schemaKey: schema # The payload schema key (payload.schema) dataKey: data # The payload data key (payload.data)","title":"\ud83d\udfe2 Self-Describing Events"},{"location":"inputs/self-describing/#self-describing-events","text":"","title":"\ud83d\udfe2 Self-Describing Events"},{"location":"inputs/self-describing/#collection-method","text":"Honeypot listens on a configurable endpoint for incoming POST requests of self describing payloads, structured as: { $CONTEXTS_ROOT_KEY: { \"some-context-schema\": {\"context-data\": \"here\"}, \"another-context-schema\": {\"more-context-data\": \"here\"} }, $PAYLOAD_ROOT_KEY: { $PAYLOAD_SCHEMA_KEY: \"some-key\", $PAYLOAD_DATA_KEY: {\"data\": \"here\"} } } This (configured by you!) endpoint accepts batches of self-describing events and single self-describing events . It requires a Content-Type header of application/json . Note! If a Content-Type header is not specified, the event will not be accepted.","title":"Collection Method"},{"location":"inputs/self-describing/#validation-method","text":"Honeypot uses the schema defined at $PAYLOAD_KEY.$SCHEMA_KEY to validate each payload.","title":"Validation Method"},{"location":"inputs/self-describing/#sample-self-describing-event-configuration","text":"generic: enabled: true # Whether or not to enable generic self-describing events path: /gen/p # Path for incoming self-describing events contexts: rootKey: contexts # The contexts root key (contexts) payload: rootKey: payload # The payload root key (payload) schemaKey: schema # The payload schema key (payload.schema) dataKey: data # The payload data key (payload.data)","title":"Sample Self-Describing Event Configuration"},{"location":"inputs/snowplow/","tags":["collector","input protocol","snowplow"],"text":"\ud83d\udfe2 Snowplow Protocols At the end of the day Snowplow leverages two event protocols - the original Tracker Protocol and Custom Self-Describing Events . Honeypot supports both, but does so in a way that seamlessly blends the data model of Self-Describing Events and the traditional Tracker Protocol. It also validates and redirects tracker-protocol events in the same manner as self-describing events. Collection Methods Snowplow uses two HTTP verbs for event collection: GET (request consisting of a single event, as defined via query params) POST (request consisting of event batches, as defined via json payloads) It leverages three primary event collection endpoints: /i (pixel endpoint used for GET -based tracking) r/tp2 (redirect endpoint used for GET -based tracking) com.snowplow.analytics.snowplow/tp2 (endpoint used for POST -based tracking) Honeypot supports all of the above, but allows various functionality to be enabled/disabled as needed. Validation Method Honeypot uses a set of static schemas to validate incoming \"tracker protocol\" events, and the associated schema property to validate incoming \"self describing\" events. Navigating Ad Blockers Honeypot supports configurable collection endpoints so tracking does not get blocked by ever-expanding ad blocker lists. Sample Snowplow Configuration inputs: snowplow: enabled: true # Whether or not to enable Snowplow event collection standardRoutesEnabled: true # Whether or not to enable Snowplow's standard routes openRedirectsEnabled: true # Whether or not to enable open redirects getPath: /plw/g # The custom path for get-based tracking postPath: /plw/p # The custom path for post-based tracking redirectPath: /plw/r # The custom path for open redirect tracking anonymize: ip: false # Whether or not to anonymize ip's userId: false # Whether or not to anonymize userid's","title":"\ud83d\udfe2 Snowplow"},{"location":"inputs/snowplow/#snowplow","text":"","title":"\ud83d\udfe2 Snowplow"},{"location":"inputs/snowplow/#protocols","text":"At the end of the day Snowplow leverages two event protocols - the original Tracker Protocol and Custom Self-Describing Events . Honeypot supports both, but does so in a way that seamlessly blends the data model of Self-Describing Events and the traditional Tracker Protocol. It also validates and redirects tracker-protocol events in the same manner as self-describing events.","title":"Protocols"},{"location":"inputs/snowplow/#collection-methods","text":"Snowplow uses two HTTP verbs for event collection: GET (request consisting of a single event, as defined via query params) POST (request consisting of event batches, as defined via json payloads) It leverages three primary event collection endpoints: /i (pixel endpoint used for GET -based tracking) r/tp2 (redirect endpoint used for GET -based tracking) com.snowplow.analytics.snowplow/tp2 (endpoint used for POST -based tracking) Honeypot supports all of the above, but allows various functionality to be enabled/disabled as needed.","title":"Collection Methods"},{"location":"inputs/snowplow/#validation-method","text":"Honeypot uses a set of static schemas to validate incoming \"tracker protocol\" events, and the associated schema property to validate incoming \"self describing\" events.","title":"Validation Method"},{"location":"inputs/snowplow/#navigating-ad-blockers","text":"Honeypot supports configurable collection endpoints so tracking does not get blocked by ever-expanding ad blocker lists.","title":"Navigating Ad Blockers"},{"location":"inputs/snowplow/#sample-snowplow-configuration","text":"inputs: snowplow: enabled: true # Whether or not to enable Snowplow event collection standardRoutesEnabled: true # Whether or not to enable Snowplow's standard routes openRedirectsEnabled: true # Whether or not to enable open redirects getPath: /plw/g # The custom path for get-based tracking postPath: /plw/p # The custom path for post-based tracking redirectPath: /plw/r # The custom path for open redirect tracking anonymize: ip: false # Whether or not to anonymize ip's userId: false # Whether or not to anonymize userid's","title":"Sample Snowplow Configuration"},{"location":"inputs/webhook/","tags":["collector","input protocol","webhook"],"text":"\ud83d\udfe2 Webhook Collection Method Honeypot is capable of collecting both named and unnamed webhooks. This is designed so webhooks from multiple sources can be fired into a single set of collection infrastructure, and the sources will retain distinct identifiers. Named webhooks Named webhooks are the recommended approach and are relatively simple - the url path following what is specified in the webhook config block becomes the identifier for all webhooks sent there. For example: you have configured the webhook path to be /pooh-bear via: webhook: path: /pooh-bear Any webhook payloads fired to /pooh-bear/revenue/stripe/v1 will be identified as revenue/stripe/v1 , payloads fired to /pooh-bear/gitlab will be identified as gitlab , payloads fired to /pooh-bear/d016bb00-02db-4cc6-9852-e29c7cf3aa57 will be identified as d016bb00-02db-4cc6-9852-e29c7cf3aa57 , etc.... Unnamed webhooks Unnamed webhooks are the fallback, but this functionality comes with the inevitability of creating a pile of random json with limited context. Which you probably don't want! So use at your own risk. Unnamed webhooks are also relatively straight-forward - the configured url path in the webhook config block acts as an unnamed catch-all. For example: you have configured the webhook path to be /christopher-robbin via: webhook: path: /christopher-robbin Any webhook payloads fired to /christopher-robbin will be collected and passed along as \"unnamed webhooks\". Validation Method Honeypot does not validate incoming webhooks - they are assumed \"ok\". Sample Webhook Configuration webhook: enabled: true # Whether or not to enable webhook path: /wb/hk # Path for incoming webhooks","title":"\ud83d\udfe2 Webhook"},{"location":"inputs/webhook/#webhook","text":"","title":"\ud83d\udfe2 Webhook"},{"location":"inputs/webhook/#collection-method","text":"Honeypot is capable of collecting both named and unnamed webhooks. This is designed so webhooks from multiple sources can be fired into a single set of collection infrastructure, and the sources will retain distinct identifiers.","title":"Collection Method"},{"location":"inputs/webhook/#named-webhooks","text":"Named webhooks are the recommended approach and are relatively simple - the url path following what is specified in the webhook config block becomes the identifier for all webhooks sent there. For example: you have configured the webhook path to be /pooh-bear via: webhook: path: /pooh-bear Any webhook payloads fired to /pooh-bear/revenue/stripe/v1 will be identified as revenue/stripe/v1 , payloads fired to /pooh-bear/gitlab will be identified as gitlab , payloads fired to /pooh-bear/d016bb00-02db-4cc6-9852-e29c7cf3aa57 will be identified as d016bb00-02db-4cc6-9852-e29c7cf3aa57 , etc....","title":"Named webhooks"},{"location":"inputs/webhook/#unnamed-webhooks","text":"Unnamed webhooks are the fallback, but this functionality comes with the inevitability of creating a pile of random json with limited context. Which you probably don't want! So use at your own risk. Unnamed webhooks are also relatively straight-forward - the configured url path in the webhook config block acts as an unnamed catch-all. For example: you have configured the webhook path to be /christopher-robbin via: webhook: path: /christopher-robbin Any webhook payloads fired to /christopher-robbin will be collected and passed along as \"unnamed webhooks\".","title":"Unnamed webhooks"},{"location":"inputs/webhook/#validation-method","text":"Honeypot does not validate incoming webhooks - they are assumed \"ok\".","title":"Validation Method"},{"location":"inputs/webhook/#sample-webhook-configuration","text":"webhook: enabled: true # Whether or not to enable webhook path: /wb/hk # Path for incoming webhooks","title":"Sample Webhook Configuration"},{"location":"middleware/advancing-cookie/","tags":["middleware","advancing cookie"],"text":"\ud83d\udfe2 Advancing Cookie The advancing cookie middleware sets a server-side identity cookie when enabled. It is used to track across authentication boundaries, or to roll up events and activity sessions to a single user regardless of auth status. Sample Advancing Cookie Middleware Configuration cookie: enabled: true name: nuid secure: true ttlDays: 365 domain: localhost path: / sameSite: Lax","title":"\ud83d\udfe2 Advancing Cookie"},{"location":"middleware/advancing-cookie/#advancing-cookie","text":"The advancing cookie middleware sets a server-side identity cookie when enabled. It is used to track across authentication boundaries, or to roll up events and activity sessions to a single user regardless of auth status.","title":"\ud83d\udfe2 Advancing Cookie"},{"location":"middleware/advancing-cookie/#sample-advancing-cookie-middleware-configuration","text":"cookie: enabled: true name: nuid secure: true ttlDays: 365 domain: localhost path: / sameSite: Lax","title":"Sample Advancing Cookie Middleware Configuration"},{"location":"middleware/cors/","tags":["middleware","cors"],"text":"\ud83d\udfe2 CORS The cors middleware ensures Honeypot is able to track events across a set of domains. It allows the following headers to be entirely configurable: Access-Control-Allow-Origin Access-Control-Allow-Credentials Access-Control-Allow-Headers Access-Control-Allow-Methods Access-Control-Max-Age Sample CORS Middleware Configuration cors: enabled: true allowOrigin: - \"*\" allowCredentials: true allowMethods: - POST - OPTIONS - GET maxAge: 86400","title":"\ud83d\udfe2 CORS"},{"location":"middleware/cors/#cors","text":"The cors middleware ensures Honeypot is able to track events across a set of domains. It allows the following headers to be entirely configurable: Access-Control-Allow-Origin Access-Control-Allow-Credentials Access-Control-Allow-Headers Access-Control-Allow-Methods Access-Control-Max-Age","title":"\ud83d\udfe2 CORS"},{"location":"middleware/cors/#sample-cors-middleware-configuration","text":"cors: enabled: true allowOrigin: - \"*\" allowCredentials: true allowMethods: - POST - OPTIONS - GET maxAge: 86400","title":"Sample CORS Middleware Configuration"},{"location":"middleware/rate-limiter/","tags":["middleware","rate limiter"],"text":"\ud83d\udfe2 Rate Limiter The rate limiter middleware allows the operator to throttle incoming events from specific sources. It is not intended to be the sole mechanism by which Honeypot is protected from malicious activity, but it does help. The ratelimiter middleware returns a 429 if the configured threshold is exceeded by a single IP address. Sample Rate Limiter Middleware Configuration rateLimiter: enabled: false period: S limit: 10","title":"\ud83d\udfe2 Rate Limiter"},{"location":"middleware/rate-limiter/#rate-limiter","text":"The rate limiter middleware allows the operator to throttle incoming events from specific sources. It is not intended to be the sole mechanism by which Honeypot is protected from malicious activity, but it does help. The ratelimiter middleware returns a 429 if the configured threshold is exceeded by a single IP address.","title":"\ud83d\udfe2 Rate Limiter"},{"location":"middleware/rate-limiter/#sample-rate-limiter-middleware-configuration","text":"rateLimiter: enabled: false period: S limit: 10","title":"Sample Rate Limiter Middleware Configuration"},{"location":"middleware/request-logger/","tags":["middleware","request logger"],"text":"\ud83d\udfe2 Request Logger The request logger middleware does exactly what it sounds like! It logs requests in the following form: {\"level\":\"info\",\"request\":{\"responseCode\":200,\"requestDuration\":7412000,\"requestDurationForHumans\":\"7.412ms\",\"clientIp\":\"127.0.0.1:59221\",\"requestMethod\":\"POST\",\"requestUri\":\"/com.snowplowanalytics.snowplow/tp2\"},\"time\":\"2022-04-28T00:58:15-04:00\"} Sample Request Logger Configuration requestLogger: enabled: true","title":"\ud83d\udfe2 Request Logger"},{"location":"middleware/request-logger/#request-logger","text":"The request logger middleware does exactly what it sounds like! It logs requests in the following form: {\"level\":\"info\",\"request\":{\"responseCode\":200,\"requestDuration\":7412000,\"requestDurationForHumans\":\"7.412ms\",\"clientIp\":\"127.0.0.1:59221\",\"requestMethod\":\"POST\",\"requestUri\":\"/com.snowplowanalytics.snowplow/tp2\"},\"time\":\"2022-04-28T00:58:15-04:00\"}","title":"\ud83d\udfe2 Request Logger"},{"location":"middleware/request-logger/#sample-request-logger-configuration","text":"requestLogger: enabled: true","title":"Sample Request Logger Configuration"},{"location":"middleware/timeout/","tags":["middleware","timeout"],"text":"\ud83d\udfe2 Request Timeout This request timeout middleware allows the Honeypot operator to explicitly declare a time threshold, in milliseconds, after which a request times out. If a request is in flight longer than the configured threshold a 408 is returned. Sample Timeout Middleware Configuration middleware: timeout: enabled: false ms: 2000","title":"\ud83d\udfe2 Request Timeout"},{"location":"middleware/timeout/#request-timeout","text":"This request timeout middleware allows the Honeypot operator to explicitly declare a time threshold, in milliseconds, after which a request times out. If a request is in flight longer than the configured threshold a 408 is returned.","title":"\ud83d\udfe2 Request Timeout"},{"location":"middleware/timeout/#sample-timeout-middleware-configuration","text":"middleware: timeout: enabled: false ms: 2000","title":"Sample Timeout Middleware Configuration"},{"location":"middleware/uid2/","tags":["middleware","uid2.0","unified id 2.0"],"text":"\ud83d\udfe1 Unified ID 2.0 Unified ID 2.0 is not yet supported.","title":"\ud83d\udfe1 Unified ID 2.0"},{"location":"middleware/uid2/#unified-id-20","text":"Unified ID 2.0 is not yet supported.","title":"\ud83d\udfe1 Unified ID 2.0"},{"location":"middleware/yeet/","tags":["middleware"],"text":"\ud83d\udfe1 Yeet Yeet middleware is not yet supported. Stay tuned","title":"\ud83d\udfe1 Yeet"},{"location":"middleware/yeet/#yeet","text":"Yeet middleware is not yet supported. Stay tuned","title":"\ud83d\udfe1 Yeet"},{"location":"quickstart/getting-started/","tags":["getting started","redpanda","kafka","kowl","materialize"],"text":"Event Streaming with \ud83c\udf6f This quickstart will get you started with Honeypot, a multi-node Redpanda cluster, Kowl, and Materialize for rapidly bootstrapping streaming analytics. It is an end-to-end real-time event collection, pipelining, and aggregation system. Quickstart (with Docker) Note! If you don't have docker and docker-compose , you will need to install those first. rpk is a nice tool from the fine folks at redpanda. mzcli and psql are nice tools to have if you want to query Materialize. 1. Clone the Honeypot repo git clone git@github.com:silverton-io/honeypot.git && cd honeypot ~/code \u276f\u276f\u276f git clone git@github.com:silverton-io/honeypot.git Cloning into 'honeypot'... remote: Enumerating objects: 1324, done. remote: Counting objects: 100% (1324/1324), done. remote: Compressing objects: 100% (615/615), done. remote: Total 1324 (delta 611), reused 1163 (delta 495), pack-reused 0 Receiving objects: 100% (1324/1324), 25.89 MiB | 4.35 MiB/s, done. Resolving deltas: 100% (611/611), done. ~/code \u276f\u276f\u276f cd honeypot 2. Bootstrap Honeypot, Redpanda, Kowl, and Materialize docker-compose -f examples/quickstart/docker-compose.yml up -d Note: This step requires docker-compose . ~/c/honeypot \u276f\u276f\u276f docker-compose -f examples/quickstart/docker-compose.yml up -d ... ... ... \u283f Container mzcli Started 4.3s \u283f Container redpanda-1 Started 4.6s \u283f Container materialized Started 4.6s \u283f Container redpanda-3 Started 4.4s \u283f Container redpanda-2 Started 4.5s \u283f Container redpanda-init Started 6.3s \u283f Container kowl Started 6.4s \u283f Container honeypot Started 7.1s ~/c/honeypot \u276f\u276f\u276f 3. Create Kafka Topics, Materialize Sources and Materialized Views Note: This step requires rpk and psql . ./examples/quickstart/setup.sh ~/c/honeypot \u276f\u276f\u276f ./examples/quickstart/setup.sh TOPIC STATUS hpt-invalid OK TOPIC STATUS hpt-valid OK CREATE SOURCE CREATE SOURCE CREATE VIEW CREATE VIEW 4. Send Events to Honeypot The Honeypot quickstart serves a page that you can use to quickly start firing Snowplow events using the Snowplow Browser Tracker here: localhost:8080 The following events are tracked on this page: Page views Page ping (time on page) Forms Redirects Transactions and transaction items Struct events Snowplow self-describing events using both the GET and batch POST capabilities of the Snowplow tracker. 5. View Events in the Kowl UI The Quickstart runs Kowl at localhost:8081 . Incoming events can be viewed in Kowl using the Topic Viewer . 6. Query Real-Time Events using a Streaming Materialized View Connect to Materialize using psql : psql -h 127.0.0.1 -p 6875 -U materialize And select real-time data from the valid_events and invalid_events materialized views: ~/c/honeypot \u276f\u276f\u276f psql -h 127.0.0.1 -p 6875 -U materialize psql (14.1, server 9.5.0) Type \"help\" for help. materialize=> \\x Expanded display is on. materialize=> select * from valid_events limit 1; -[ RECORD 1 ]-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- event | {\"app_id\":\"site\",\"br_colordepth\":24,\"br_cookies\":true,\"br_features_director\":false,\"br_features_flash\":false,\"br_features_gears\":false,\"br_features_java\":false,\"br_features_pdf\":false,\"br_features_quicktime\":false,\"br_features_realplayer\":false,\"br_features_silverlight\":false,\"br_features_windowsmedia\":false,\"br_lang\":\"en-US\",\"br_viewheight\":891,\"br_viewwidth\":1439,\"collector_tstamp\":\"2022-03-09T22:01:46.26821984Z\",\"contexts\":[{\"data\":{\"id\":\"4b6cd308-b363-418a-9935-4fc1fa7e40aa\"},\"schema\":\"iglu:com.snowplowanalytics.snowplow/web_page/jsonschema/1-0-0\"},{\"data\":{\"connectEnd\":1646863299964,\"connectStart\":1646863299964,\"domComplete\":1646863299987,\"domContentLoadedEventEnd\":1646863299973,\"domContentLoadedEventStart\":1646863299973,\"domInteractive\":1646863299973,\"domLoading\":1646863299967,\"domainLookupEnd\":1646863299964,\"domainLookupStart\":1646863299964,\"fetchStart\":1646863299964,\"loadEventEnd\":1646863299987,\"loadEventStart\":1646863299987,\"navigationStart\":1646863299962,\"redirectEnd\":0,\"redirectStart\":0,\"requestStart\":1646863299964,\"responseEnd\":1646863299966,\"responseStart\":1646863299965,\"secureConnectionStart\":0,\"unloadEventEnd\":0,\"unloadEventStart\":0},\"schema\":\"iglu:org.w3/PerformanceTiming/jsonschema/1-0-0\"},{\"data\":{\"brands\":[{\"brand\":\" Not A;Brand\",\"version\":\"99\"},{\"brand\":\"Chromium\",\"version\":\"99\"},{\"brand\":\"Google Chrome\",\"version\":\"99\"}],\"isMobile\":false},\"schema\":\"iglu:org.ietf/http_client_hints/jsonschema/1-0-0\"}],\"derived_tstamp\":\"2022-03-09T22:01:45.57121984Z\",\"doc_charset\":\"UTF-8\",\"doc_height\":963,\"doc_size\":\"1439x963\",\"doc_width\":1439,\"domain_sessionid\":\"f6a93db1-e68b-480a-9779-37c51bb1edd2\",\"domain_sessionidx\":6,\"domain_userid\":\"aed04c37-23a8-4877-9fc8-505e9e2ee9a8\",\"dvce_created_tstamp\":\"2022-03-09T22:01:45.56Z\",\"dvce_screenheight\":1080,\"dvce_screenwidth\":1920,\"dvce_sent_tstamp\":\"2022-03-09T22:01:46.257Z\",\"etl_tstamp\":\"2022-03-09T22:01:46.268219882Z\",\"event\":\"page_view\",\"event_fingerprint\":null,\"event_format\":\"\",\"event_id\":\"95ffadf6-9301-40bf-b2de-f80d4511b851\",\"event_name\":\"\",\"event_vendor\":\"\",\"event_version\":\"\",\"mac_address\":null,\"mkt_campaign\":\"\",\"mkt_content\":\"\",\"mkt_medium\":\"\",\"mkt_source\":\"\",\"mkt_term\":\"\",\"monitor_resolution\":\"1920x1080\",\"name_tracker\":\"sp\",\"network_userid\":\"\",\"os_timezone\":\"America/New_York\",\"page_referrer\":null,\"page_title\":null,\"page_url\":\"http://localhost:8080/\",\"page_urlfragment\":\"\",\"page_urlhost\":\"localhost:8080\",\"page_urlpath\":\"/\",\"page_urlport\":null,\"page_urlquery\":\"\",\"page_urlscheme\":\"http\",\"platform\":\"web\",\"pp_xoffset_max\":null,\"pp_xoffset_min\":null,\"pp_yoffset_max\":null,\"pp_yoffset_min\":null,\"refr_campaign\":null,\"refr_content\":null,\"refr_domain_tstamp\":null,\"refr_domain_userid\":null,\"refr_medium\":null,\"refr_source\":null,\"refr_term\":null,\"refr_urlfragment\":null,\"refr_urlhost\":null,\"refr_urlpath\":null,\"refr_urlport\":null,\"refr_urlquery\":null,\"refr_urlscheme\":null,\"se_action\":null,\"se_category\":null,\"se_label\":null,\"se_property\":null,\"se_value\":null,\"self_describing_event\":null,\"ti_category\":null,\"ti_currency\":null,\"ti_name\":null,\"ti_orderid\":null,\"ti_price\":null,\"ti_quantity\":null,\"ti_sku\":null,\"tr_affiliation\":null,\"tr_city\":null,\"tr_country\":null,\"tr_currency\":null,\"tr_orderid\":null,\"tr_shipping\":null,\"tr_state\":null,\"tr_tax\":null,\"tr_total\":null,\"true_tstamp\":null,\"txn_id\":null,\"user_fingerprint\":null,\"user_id\":null,\"user_ipaddress\":\"3b1412753f475cc969c37231dd6eaea2\",\"useragent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\",\"v_collector\":\"v0.1.21\",\"v_etl\":\"v0.1.21\",\"v_tracker\":\"js-3.2.3\",\"viewport_size\":\"1439x891\"} materialize=> Next Steps We have a lot planned for the coming months. Want to be a part of it? Sign up for Insiders-Only Updates Check out the Honeypot Roadmap Component Overview Honeypot Honeypot is a multi-protocol event collection, validation, and routing system. Want to track Snowplow events? Think Honeypot. Want to track Cloudevents? Think Honeypot. Have a more custom, self-describing event implementation? Think Honeypot. Redpanda Redpanda is Kafka-compatible streaming platform, with 100% less Zookeeper and JVM. It is blazing fast, quick to set up, and incredibly operator-oriented. This example uses a three-node Redpanda cluster as the streaming interface between Honeypot and Materialize. Redpanda docs can be found here: docs.redpanda.com . Kowl Kowl is a very useful UI for Kafka cluster visibility, schema discovery, and other administrative tasks. Honeypot quickstart uses it to quickly visualize and verify data flowing through Redpanda (on its way to Materialize). More on Kowl can be found here: cloudhut.dev Materialize Materialize is a streaming, SQL-based materialized view engine based on Differential dataflow . This example uses Materialize to create real-time aggregates and activity funnels by streaming data from Honeypot, through Redpanda, into a Materialize Source, before aggregating in a materialized view.","title":"Event Streaming with \ud83c\udf6f"},{"location":"quickstart/getting-started/#event-streaming-with","text":"This quickstart will get you started with Honeypot, a multi-node Redpanda cluster, Kowl, and Materialize for rapidly bootstrapping streaming analytics. It is an end-to-end real-time event collection, pipelining, and aggregation system.","title":"Event Streaming with \ud83c\udf6f"},{"location":"quickstart/getting-started/#quickstart-with-docker","text":"Note! If you don't have docker and docker-compose , you will need to install those first. rpk is a nice tool from the fine folks at redpanda. mzcli and psql are nice tools to have if you want to query Materialize.","title":"Quickstart (with Docker)"},{"location":"quickstart/getting-started/#1-clone-the-honeypot-repo","text":"git clone git@github.com:silverton-io/honeypot.git && cd honeypot ~/code \u276f\u276f\u276f git clone git@github.com:silverton-io/honeypot.git Cloning into 'honeypot'... remote: Enumerating objects: 1324, done. remote: Counting objects: 100% (1324/1324), done. remote: Compressing objects: 100% (615/615), done. remote: Total 1324 (delta 611), reused 1163 (delta 495), pack-reused 0 Receiving objects: 100% (1324/1324), 25.89 MiB | 4.35 MiB/s, done. Resolving deltas: 100% (611/611), done. ~/code \u276f\u276f\u276f cd honeypot","title":"1. Clone the Honeypot repo"},{"location":"quickstart/getting-started/#2-bootstrap-honeypot-redpanda-kowl-and-materialize","text":"docker-compose -f examples/quickstart/docker-compose.yml up -d Note: This step requires docker-compose . ~/c/honeypot \u276f\u276f\u276f docker-compose -f examples/quickstart/docker-compose.yml up -d ... ... ... \u283f Container mzcli Started 4.3s \u283f Container redpanda-1 Started 4.6s \u283f Container materialized Started 4.6s \u283f Container redpanda-3 Started 4.4s \u283f Container redpanda-2 Started 4.5s \u283f Container redpanda-init Started 6.3s \u283f Container kowl Started 6.4s \u283f Container honeypot Started 7.1s ~/c/honeypot \u276f\u276f\u276f","title":"2. Bootstrap Honeypot, Redpanda, Kowl, and Materialize"},{"location":"quickstart/getting-started/#3-create-kafka-topics-materialize-sources-and-materialized-views","text":"Note: This step requires rpk and psql . ./examples/quickstart/setup.sh ~/c/honeypot \u276f\u276f\u276f ./examples/quickstart/setup.sh TOPIC STATUS hpt-invalid OK TOPIC STATUS hpt-valid OK CREATE SOURCE CREATE SOURCE CREATE VIEW CREATE VIEW","title":"3. Create Kafka Topics, Materialize Sources and Materialized Views"},{"location":"quickstart/getting-started/#4-send-events-to-honeypot","text":"The Honeypot quickstart serves a page that you can use to quickly start firing Snowplow events using the Snowplow Browser Tracker here: localhost:8080 The following events are tracked on this page: Page views Page ping (time on page) Forms Redirects Transactions and transaction items Struct events Snowplow self-describing events using both the GET and batch POST capabilities of the Snowplow tracker.","title":"4. Send Events to Honeypot"},{"location":"quickstart/getting-started/#5-view-events-in-the-kowl-ui","text":"The Quickstart runs Kowl at localhost:8081 . Incoming events can be viewed in Kowl using the Topic Viewer .","title":"5. View Events in the Kowl UI"},{"location":"quickstart/getting-started/#6-query-real-time-events-using-a-streaming-materialized-view","text":"Connect to Materialize using psql : psql -h 127.0.0.1 -p 6875 -U materialize And select real-time data from the valid_events and invalid_events materialized views: ~/c/honeypot \u276f\u276f\u276f psql -h 127.0.0.1 -p 6875 -U materialize psql (14.1, server 9.5.0) Type \"help\" for help. materialize=> \\x Expanded display is on. materialize=> select * from valid_events limit 1; -[ RECORD 1 ]-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- event | {\"app_id\":\"site\",\"br_colordepth\":24,\"br_cookies\":true,\"br_features_director\":false,\"br_features_flash\":false,\"br_features_gears\":false,\"br_features_java\":false,\"br_features_pdf\":false,\"br_features_quicktime\":false,\"br_features_realplayer\":false,\"br_features_silverlight\":false,\"br_features_windowsmedia\":false,\"br_lang\":\"en-US\",\"br_viewheight\":891,\"br_viewwidth\":1439,\"collector_tstamp\":\"2022-03-09T22:01:46.26821984Z\",\"contexts\":[{\"data\":{\"id\":\"4b6cd308-b363-418a-9935-4fc1fa7e40aa\"},\"schema\":\"iglu:com.snowplowanalytics.snowplow/web_page/jsonschema/1-0-0\"},{\"data\":{\"connectEnd\":1646863299964,\"connectStart\":1646863299964,\"domComplete\":1646863299987,\"domContentLoadedEventEnd\":1646863299973,\"domContentLoadedEventStart\":1646863299973,\"domInteractive\":1646863299973,\"domLoading\":1646863299967,\"domainLookupEnd\":1646863299964,\"domainLookupStart\":1646863299964,\"fetchStart\":1646863299964,\"loadEventEnd\":1646863299987,\"loadEventStart\":1646863299987,\"navigationStart\":1646863299962,\"redirectEnd\":0,\"redirectStart\":0,\"requestStart\":1646863299964,\"responseEnd\":1646863299966,\"responseStart\":1646863299965,\"secureConnectionStart\":0,\"unloadEventEnd\":0,\"unloadEventStart\":0},\"schema\":\"iglu:org.w3/PerformanceTiming/jsonschema/1-0-0\"},{\"data\":{\"brands\":[{\"brand\":\" Not A;Brand\",\"version\":\"99\"},{\"brand\":\"Chromium\",\"version\":\"99\"},{\"brand\":\"Google Chrome\",\"version\":\"99\"}],\"isMobile\":false},\"schema\":\"iglu:org.ietf/http_client_hints/jsonschema/1-0-0\"}],\"derived_tstamp\":\"2022-03-09T22:01:45.57121984Z\",\"doc_charset\":\"UTF-8\",\"doc_height\":963,\"doc_size\":\"1439x963\",\"doc_width\":1439,\"domain_sessionid\":\"f6a93db1-e68b-480a-9779-37c51bb1edd2\",\"domain_sessionidx\":6,\"domain_userid\":\"aed04c37-23a8-4877-9fc8-505e9e2ee9a8\",\"dvce_created_tstamp\":\"2022-03-09T22:01:45.56Z\",\"dvce_screenheight\":1080,\"dvce_screenwidth\":1920,\"dvce_sent_tstamp\":\"2022-03-09T22:01:46.257Z\",\"etl_tstamp\":\"2022-03-09T22:01:46.268219882Z\",\"event\":\"page_view\",\"event_fingerprint\":null,\"event_format\":\"\",\"event_id\":\"95ffadf6-9301-40bf-b2de-f80d4511b851\",\"event_name\":\"\",\"event_vendor\":\"\",\"event_version\":\"\",\"mac_address\":null,\"mkt_campaign\":\"\",\"mkt_content\":\"\",\"mkt_medium\":\"\",\"mkt_source\":\"\",\"mkt_term\":\"\",\"monitor_resolution\":\"1920x1080\",\"name_tracker\":\"sp\",\"network_userid\":\"\",\"os_timezone\":\"America/New_York\",\"page_referrer\":null,\"page_title\":null,\"page_url\":\"http://localhost:8080/\",\"page_urlfragment\":\"\",\"page_urlhost\":\"localhost:8080\",\"page_urlpath\":\"/\",\"page_urlport\":null,\"page_urlquery\":\"\",\"page_urlscheme\":\"http\",\"platform\":\"web\",\"pp_xoffset_max\":null,\"pp_xoffset_min\":null,\"pp_yoffset_max\":null,\"pp_yoffset_min\":null,\"refr_campaign\":null,\"refr_content\":null,\"refr_domain_tstamp\":null,\"refr_domain_userid\":null,\"refr_medium\":null,\"refr_source\":null,\"refr_term\":null,\"refr_urlfragment\":null,\"refr_urlhost\":null,\"refr_urlpath\":null,\"refr_urlport\":null,\"refr_urlquery\":null,\"refr_urlscheme\":null,\"se_action\":null,\"se_category\":null,\"se_label\":null,\"se_property\":null,\"se_value\":null,\"self_describing_event\":null,\"ti_category\":null,\"ti_currency\":null,\"ti_name\":null,\"ti_orderid\":null,\"ti_price\":null,\"ti_quantity\":null,\"ti_sku\":null,\"tr_affiliation\":null,\"tr_city\":null,\"tr_country\":null,\"tr_currency\":null,\"tr_orderid\":null,\"tr_shipping\":null,\"tr_state\":null,\"tr_tax\":null,\"tr_total\":null,\"true_tstamp\":null,\"txn_id\":null,\"user_fingerprint\":null,\"user_id\":null,\"user_ipaddress\":\"3b1412753f475cc969c37231dd6eaea2\",\"useragent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\",\"v_collector\":\"v0.1.21\",\"v_etl\":\"v0.1.21\",\"v_tracker\":\"js-3.2.3\",\"viewport_size\":\"1439x891\"} materialize=>","title":"6. Query Real-Time Events using a Streaming Materialized View"},{"location":"quickstart/getting-started/#next-steps","text":"We have a lot planned for the coming months. Want to be a part of it? Sign up for Insiders-Only Updates Check out the Honeypot Roadmap","title":"Next Steps"},{"location":"quickstart/getting-started/#component-overview","text":"","title":"Component Overview"},{"location":"quickstart/getting-started/#honeypot","text":"Honeypot is a multi-protocol event collection, validation, and routing system. Want to track Snowplow events? Think Honeypot. Want to track Cloudevents? Think Honeypot. Have a more custom, self-describing event implementation? Think Honeypot.","title":"Honeypot"},{"location":"quickstart/getting-started/#redpanda","text":"Redpanda is Kafka-compatible streaming platform, with 100% less Zookeeper and JVM. It is blazing fast, quick to set up, and incredibly operator-oriented. This example uses a three-node Redpanda cluster as the streaming interface between Honeypot and Materialize. Redpanda docs can be found here: docs.redpanda.com .","title":"Redpanda"},{"location":"quickstart/getting-started/#kowl","text":"Kowl is a very useful UI for Kafka cluster visibility, schema discovery, and other administrative tasks. Honeypot quickstart uses it to quickly visualize and verify data flowing through Redpanda (on its way to Materialize). More on Kowl can be found here: cloudhut.dev","title":"Kowl"},{"location":"quickstart/getting-started/#materialize","text":"Materialize is a streaming, SQL-based materialized view engine based on Differential dataflow . This example uses Materialize to create real-time aggregates and activity funnels by streaming data from Honeypot, through Redpanda, into a Materialize Source, before aggregating in a materialized view.","title":"Materialize"},{"location":"quickstart/honeypot-and-benthos/","tags":["getting started","benthos"],"text":"Benthos Stream Processing","title":"Benthos Stream Processing"},{"location":"quickstart/honeypot-and-benthos/#benthos-stream-processing","text":"","title":"Benthos Stream Processing"},{"location":"roadmap/2022/","tags":["roadmap",2022],"text":"\ud83c\udfaf 2022 We have three goals for 2022. Core Functionality Deployment Experience Development Experience Do you want be informed of our progress? Sign up for Insiders-Only Updates . \ud83c\udf6f\ud83e\udd42","title":"\ud83c\udfaf 2022"},{"location":"roadmap/2022/#2022","text":"We have three goals for 2022. Core Functionality Deployment Experience Development Experience","title":"\ud83c\udfaf 2022"},{"location":"roadmap/2022/#do-you-want-be-informed-of-our-progress-sign-up-for-insiders-only-updates","text":"\ud83c\udf6f\ud83e\udd42","title":"Do you want be informed of our progress? Sign up for Insiders-Only Updates."},{"location":"schema-cache/onboard/","tags":["schema cache","onboard"],"text":"\ud83d\udfe2 Onboard In order to drastically improve event validation speed, schemas are cached onboard each running honeypot for the configure period of time. Schema Directory If the schemaDirectory is enabled , all \"currently-cached\" schemas are available at the /schemas path of the collector. Individual schemas are also available at /schemas/$NAME/$OF/$SCHEMA . For example, if a particular schema had a name of io.silverton/honeypot/tele/beat/v1.0.json it would be available at /schemas/io.silverton/honeypot/tele/beat/v1.0.json . Sample Schema Cache Configuration schemaCache: ttlSeconds: 300 # The number of seconds to keep a schema in the onboard cache maxSizeBytes: 104857600 # The max size, in bytes, of the onboard cache purge: enabled: true # Whether or not to enable a cache purge route path: /c/purge # The path of the cache purge route schemaDirectory: enabled: true # Whether or not to enable schema directory routes","title":"\ud83d\udfe2 Onboard"},{"location":"schema-cache/onboard/#onboard","text":"In order to drastically improve event validation speed, schemas are cached onboard each running honeypot for the configure period of time.","title":"\ud83d\udfe2 Onboard"},{"location":"schema-cache/onboard/#schema-directory","text":"If the schemaDirectory is enabled , all \"currently-cached\" schemas are available at the /schemas path of the collector. Individual schemas are also available at /schemas/$NAME/$OF/$SCHEMA . For example, if a particular schema had a name of io.silverton/honeypot/tele/beat/v1.0.json it would be available at /schemas/io.silverton/honeypot/tele/beat/v1.0.json .","title":"Schema Directory"},{"location":"schema-cache/onboard/#sample-schema-cache-configuration","text":"schemaCache: ttlSeconds: 300 # The number of seconds to keep a schema in the onboard cache maxSizeBytes: 104857600 # The max size, in bytes, of the onboard cache purge: enabled: true # Whether or not to enable a cache purge route path: /c/purge # The path of the cache purge route schemaDirectory: enabled: true # Whether or not to enable schema directory routes","title":"Sample Schema Cache Configuration"},{"location":"schema-cache/redis/","tags":["schema cache","redis"],"text":"\ud83d\udfe1 Redis Redis-based schema caching is not yet supported. See this Github Issue for current status.","title":"\ud83d\udfe1 Redis"},{"location":"schema-cache/redis/#redis","text":"Redis-based schema caching is not yet supported. See this Github Issue for current status.","title":"\ud83d\udfe1 Redis"},{"location":"schema-cache-backends/clickhouse/","tags":["schema cache backend","db","clickhouse"],"text":"\ud83d\udfe2 Clickhouse The clickhouse schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s). Sample Clickhouse Schema Cache Backend Configuration schemaCache: backend: type: clickhouse registryTable: registry clickhouseHost: 127.0.0.1 clickhousePort: 9000 clickhouseDbName: honeypot clickhouseUser: honeypot clickhousePass: honeypot","title":"\ud83d\udfe2 Clickhouse"},{"location":"schema-cache-backends/clickhouse/#clickhouse","text":"The clickhouse schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s).","title":"\ud83d\udfe2 Clickhouse"},{"location":"schema-cache-backends/clickhouse/#sample-clickhouse-schema-cache-backend-configuration","text":"schemaCache: backend: type: clickhouse registryTable: registry clickhouseHost: 127.0.0.1 clickhousePort: 9000 clickhouseDbName: honeypot clickhouseUser: honeypot clickhousePass: honeypot","title":"Sample Clickhouse Schema Cache Backend Configuration"},{"location":"schema-cache-backends/filesystem/","tags":["schema cache backend","fs","filesystem"],"text":"\ud83d\udfe2 Filesystem The fs cache backend uses jsonschemas stored on the local filesystem to back the in-memory schema cache. Sample Filesystem Schema Cache Backend Configuration schemaCache: backend: type: fs # The backend type path: /some/path/somewhere # The path to consider as root","title":"\ud83d\udfe2 Filesystem"},{"location":"schema-cache-backends/filesystem/#filesystem","text":"The fs cache backend uses jsonschemas stored on the local filesystem to back the in-memory schema cache.","title":"\ud83d\udfe2 Filesystem"},{"location":"schema-cache-backends/filesystem/#sample-filesystem-schema-cache-backend-configuration","text":"schemaCache: backend: type: fs # The backend type path: /some/path/somewhere # The path to consider as root","title":"Sample Filesystem Schema Cache Backend Configuration"},{"location":"schema-cache-backends/gcs/","tags":["schema cache backend","gcs","google cloud storage"],"text":"\ud83d\udfe2 GCS The gcs cache backend uses jsonschemas stored in gcs to back the in-memory schema cache. Sample GCS Cache Backend Configuration schemaCache: backend: type: gcs # The backend type bucket: honeypot-schemas # The gcs bucket containing schemas path: / # The path to consider as root","title":"\ud83d\udfe2 GCS"},{"location":"schema-cache-backends/gcs/#gcs","text":"The gcs cache backend uses jsonschemas stored in gcs to back the in-memory schema cache.","title":"\ud83d\udfe2 GCS"},{"location":"schema-cache-backends/gcs/#sample-gcs-cache-backend-configuration","text":"schemaCache: backend: type: gcs # The backend type bucket: honeypot-schemas # The gcs bucket containing schemas path: / # The path to consider as root","title":"Sample GCS Cache Backend Configuration"},{"location":"schema-cache-backends/http/","tags":["schema cache backend","http","https"],"text":"\ud83d\udfe2 HTTP/S The http and https cache backends use jsonschemas stored at remote HTTP paths to back the in-memory schema cache. Sample Filesystem Cache Backend Configuration schemaCache: backend: type: https # The backend type host: registry.silverton.io # The schema host path: /some/path/somewhere # The path to consider as root","title":"\ud83d\udfe2 HTTP/S"},{"location":"schema-cache-backends/http/#https","text":"The http and https cache backends use jsonschemas stored at remote HTTP paths to back the in-memory schema cache.","title":"\ud83d\udfe2 HTTP/S"},{"location":"schema-cache-backends/http/#sample-filesystem-cache-backend-configuration","text":"schemaCache: backend: type: https # The backend type host: registry.silverton.io # The schema host path: /some/path/somewhere # The path to consider as root","title":"Sample Filesystem Cache Backend Configuration"},{"location":"schema-cache-backends/iglu/","tags":["schema cache backend","snowplow","iglu"],"text":"\ud83d\udfe1 IGLU","title":"\ud83d\udfe1 IGLU"},{"location":"schema-cache-backends/iglu/#iglu","text":"","title":"\ud83d\udfe1 IGLU"},{"location":"schema-cache-backends/kafka-registry/","tags":["schema cache backend","redpanda","kafka","registry"],"text":"\ud83d\udfe1 Kafka Schema Registry The Kafka schema cache backend is not yet supported. The intent is to wait until Redpanda's registry supports jsonschema or until Honeypot validation supports Avro. TBD. See this Github Issue for current status.","title":"\ud83d\udfe1 Kafka Schema Registry"},{"location":"schema-cache-backends/kafka-registry/#kafka-schema-registry","text":"The Kafka schema cache backend is not yet supported. The intent is to wait until Redpanda's registry supports jsonschema or until Honeypot validation supports Avro. TBD. See this Github Issue for current status.","title":"\ud83d\udfe1 Kafka Schema Registry"},{"location":"schema-cache-backends/materialize/","tags":["schema cache backend","db","materialize"],"text":"\ud83d\udfe2 Materialize The materialize schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s). Sample Materialize Schema Cache Backend Configuration schemaCache: backend: type: materialize registryTable: registry mzHost: localhost mzPort: 6875 mzDbName: materialize mzUser: materialize mzPass: \"\"","title":"\ud83d\udfe2 Materialize"},{"location":"schema-cache-backends/materialize/#materialize","text":"The materialize schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s).","title":"\ud83d\udfe2 Materialize"},{"location":"schema-cache-backends/materialize/#sample-materialize-schema-cache-backend-configuration","text":"schemaCache: backend: type: materialize registryTable: registry mzHost: localhost mzPort: 6875 mzDbName: materialize mzUser: materialize mzPass: \"\"","title":"Sample Materialize Schema Cache Backend Configuration"},{"location":"schema-cache-backends/mongodb/","tags":["schema cache backend","db","mongodb"],"text":"\ud83d\udfe2 MongoDb The mongodb schema cache backend leverages schemas stored in a configurable registry collection. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s). Sample Mongodb Schema Cache Backend Configuration schemaCache: backend: type: mongodb registryCollection: registry mongoHosts: - 127.0.0.1 mongoPort: 27017 mongoDbName: honeypot mongoUser: honeypot mongoPass: honeypot","title":"\ud83d\udfe2 MongoDb"},{"location":"schema-cache-backends/mongodb/#mongodb","text":"The mongodb schema cache backend leverages schemas stored in a configurable registry collection. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s).","title":"\ud83d\udfe2 MongoDb"},{"location":"schema-cache-backends/mongodb/#sample-mongodb-schema-cache-backend-configuration","text":"schemaCache: backend: type: mongodb registryCollection: registry mongoHosts: - 127.0.0.1 mongoPort: 27017 mongoDbName: honeypot mongoUser: honeypot mongoPass: honeypot","title":"Sample Mongodb Schema Cache Backend Configuration"},{"location":"schema-cache-backends/mysql/","tags":["schema cache backend","db","mysql"],"text":"\ud83d\udfe2 MySQL The mysql schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s). Sample Mysql Schema Cache Backend Configuration schemaCache: backend: type: mysql registryTable: registry mysqlHost: localhost mysqlPort: 3306 mysqlDbName: honeypot mysqlUser: honeypot mysqlPass: honeypot","title":"\ud83d\udfe2 MySQL"},{"location":"schema-cache-backends/mysql/#mysql","text":"The mysql schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s).","title":"\ud83d\udfe2 MySQL"},{"location":"schema-cache-backends/mysql/#sample-mysql-schema-cache-backend-configuration","text":"schemaCache: backend: type: mysql registryTable: registry mysqlHost: localhost mysqlPort: 3306 mysqlDbName: honeypot mysqlUser: honeypot mysqlPass: honeypot","title":"Sample Mysql Schema Cache Backend Configuration"},{"location":"schema-cache-backends/postgres/","tags":["schema cache backend","db","postgres"],"text":"\ud83d\udfe2 Postgres The postgres schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s). Sample Postgres Schema Cache Backend Configuration schemaCache: backend: type: postgres registryTable: registry pgHost: localhost pgPort: 5432 pgDbName: honeypot pgUser: honeypot pgPass: honeypot","title":"\ud83d\udfe2 Postgres"},{"location":"schema-cache-backends/postgres/#postgres","text":"The postgres schema cache backend leverages schemas stored in a configurable registry table. It is most useful when you want to store schemas , valid events , and invalid events within the same system to reduce infrastructure overhead. It can be used with any combination of sink(s).","title":"\ud83d\udfe2 Postgres"},{"location":"schema-cache-backends/postgres/#sample-postgres-schema-cache-backend-configuration","text":"schemaCache: backend: type: postgres registryTable: registry pgHost: localhost pgPort: 5432 pgDbName: honeypot pgUser: honeypot pgPass: honeypot","title":"Sample Postgres Schema Cache Backend Configuration"},{"location":"schema-cache-backends/redis/","tags":["schema cache backend","redis"],"text":"\ud83d\udfe1 Redis The redis schema cache backend is not yet supported.","title":"\ud83d\udfe1 Redis"},{"location":"schema-cache-backends/redis/#redis","text":"The redis schema cache backend is not yet supported.","title":"\ud83d\udfe1 Redis"},{"location":"schema-cache-backends/s3/","tags":["schema cache backend","s3","aws simple storage service"],"text":"\ud83d\udfe2 S3 The s3 cache backend uses jsonschemas stored in s3 to back the in-memory schema cache. Sample S3 Cache Backend Configuration schemaCache: backend: type: s3 # The backend type bucket: honeypot-schemas # The s3 bucket containing schemas path: / # The path to consider as root","title":"\ud83d\udfe2 S3"},{"location":"schema-cache-backends/s3/#s3","text":"The s3 cache backend uses jsonschemas stored in s3 to back the in-memory schema cache.","title":"\ud83d\udfe2 S3"},{"location":"schema-cache-backends/s3/#sample-s3-cache-backend-configuration","text":"schemaCache: backend: type: s3 # The backend type bucket: honeypot-schemas # The s3 bucket containing schemas path: / # The path to consider as root","title":"Sample S3 Cache Backend Configuration"},{"location":"serialization-protocols/avro/","tags":["collector","validation protocol","avro"],"text":"\ud83d\udfe1 Avro Avro serialization is not yet supported. Stay tuned!","title":"\ud83d\udfe1 Avro"},{"location":"serialization-protocols/avro/#avro","text":"Avro serialization is not yet supported. Stay tuned!","title":"\ud83d\udfe1 Avro"},{"location":"serialization-protocols/json/","tags":["serialization","json"],"text":"\ud83d\udfe2 JSON JSON is the default serialization for all events.","title":"\ud83d\udfe2 JSON"},{"location":"serialization-protocols/json/#json","text":"JSON is the default serialization for all events.","title":"\ud83d\udfe2 JSON"},{"location":"sinks/amplitude/","tags":["sink","saas","amplitude"],"text":"\ud83d\udfe1 Amplitude Direct Amplitude sink is not yet supported.","title":"\ud83d\udfe1 Amplitude"},{"location":"sinks/amplitude/#amplitude","text":"Direct Amplitude sink is not yet supported.","title":"\ud83d\udfe1 Amplitude"},{"location":"sinks/azuredw/","tags":["sink","dw","azure data warehouse"],"text":"\ud83d\udfe1 Azure Data Warehouse Direct Azure DW sink is not yet supported.","title":"\ud83d\udfe1 Azure Data Warehouse"},{"location":"sinks/azuredw/#azure-data-warehouse","text":"Direct Azure DW sink is not yet supported.","title":"\ud83d\udfe1 Azure Data Warehouse"},{"location":"sinks/bigquery/","tags":["sink","dw","bigquery"],"text":"\ud83d\udfe1 BigQuery Direct BigQuery sink is not yet supported.","title":"\ud83d\udfe1 BigQuery"},{"location":"sinks/bigquery/#bigquery","text":"Direct BigQuery sink is not yet supported.","title":"\ud83d\udfe1 BigQuery"},{"location":"sinks/blackhole/","tags":["sink","blackhole"],"text":"\ud83d\udfe2 Blackhole The blackhole sink is the equivalent of sinking events to /dev/null . It is primarily useful as a development tool or when collecting events in non-production environments if you don't want to sink them anywhere. Sample Blackhole Sink Configuration sinks: - name: supermassive type: blackhole deliveryRequired: true","title":"\ud83d\udfe2 Blackhole"},{"location":"sinks/blackhole/#blackhole","text":"The blackhole sink is the equivalent of sinking events to /dev/null . It is primarily useful as a development tool or when collecting events in non-production environments if you don't want to sink them anywhere.","title":"\ud83d\udfe2 Blackhole"},{"location":"sinks/blackhole/#sample-blackhole-sink-configuration","text":"sinks: - name: supermassive type: blackhole deliveryRequired: true","title":"Sample Blackhole Sink Configuration"},{"location":"sinks/clickhouse/","tags":["sink","db","clickhouse"],"text":"\ud83d\udfe2 Clickhouse The Clickhouse sink loads valid and invalid events into the configured tables. Table existence is ensured each time Honeypot starts up so manual creation is not required. Sample Clickhouse Sink Configuration sinks: - name: houseofclicks type: clickhouse deliveryRequired: true clickhouseHost: 127.0.0.1 clickhousePort: 9000 clickhouseDbName: honeypot clickhouseUser: honeypot clickhousePass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"\ud83d\udfe2 Clickhouse"},{"location":"sinks/clickhouse/#clickhouse","text":"The Clickhouse sink loads valid and invalid events into the configured tables. Table existence is ensured each time Honeypot starts up so manual creation is not required.","title":"\ud83d\udfe2 Clickhouse"},{"location":"sinks/clickhouse/#sample-clickhouse-sink-configuration","text":"sinks: - name: houseofclicks type: clickhouse deliveryRequired: true clickhouseHost: 127.0.0.1 clickhousePort: 9000 clickhouseDbName: honeypot clickhouseUser: honeypot clickhousePass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"Sample Clickhouse Sink Configuration"},{"location":"sinks/databricks/","tags":["sink","dw","databricks","snowflake"],"text":"\ud83d\udfe1 Databricks Direct Databricks sink is not yet supported.","title":"\ud83d\udfe1 Databricks"},{"location":"sinks/databricks/#databricks","text":"Direct Databricks sink is not yet supported.","title":"\ud83d\udfe1 Databricks"},{"location":"sinks/elasticsearch/","tags":["sink","db","elasticsearch"],"text":"\ud83d\udfe2 Elasticsearch The Elasticsearch sink loads valid and invalid events into the configured indices. Indices are ensured via the nature of elasticsearch, so manual creation is not required. Sample Elasticsearch Sink Configuration sinks: - name: loggin type: elasticsearch deliveryRequired: true elasticsearchHosts: - http://es1:9200 elasticsearchUsername: elastic elasticsearchPassword: elastic validIndex: honeypot-valid invalidIndex: honeypot-invalid","title":"\ud83d\udfe2 Elasticsearch"},{"location":"sinks/elasticsearch/#elasticsearch","text":"The Elasticsearch sink loads valid and invalid events into the configured indices. Indices are ensured via the nature of elasticsearch, so manual creation is not required.","title":"\ud83d\udfe2 Elasticsearch"},{"location":"sinks/elasticsearch/#sample-elasticsearch-sink-configuration","text":"sinks: - name: loggin type: elasticsearch deliveryRequired: true elasticsearchHosts: - http://es1:9200 elasticsearchUsername: elastic elasticsearchPassword: elastic validIndex: honeypot-valid invalidIndex: honeypot-invalid","title":"Sample Elasticsearch Sink Configuration"},{"location":"sinks/file/","tags":["sink","file"],"text":"\ud83d\udfe2 File The file sink writes events to respective local files. It is useful, sometimes. Destination files are ensured on startup, so manual creation is not required. Sample File Sink Configuration sinks: - name: notgoingfar type: file deliveryRequired: true validFile: honeypot-valid.json invalidFile: honeypot-invalid.json","title":"\ud83d\udfe2 File"},{"location":"sinks/file/#file","text":"The file sink writes events to respective local files. It is useful, sometimes. Destination files are ensured on startup, so manual creation is not required.","title":"\ud83d\udfe2 File"},{"location":"sinks/file/#sample-file-sink-configuration","text":"sinks: - name: notgoingfar type: file deliveryRequired: true validFile: honeypot-valid.json invalidFile: honeypot-invalid.json","title":"Sample File Sink Configuration"},{"location":"sinks/firebolt/","tags":["sink","dw","firebolt"],"text":"\ud83d\udfe1 Firebolt Direct Firebolt sink is not yet supported.","title":"\ud83d\udfe1 Firebolt"},{"location":"sinks/firebolt/#firebolt","text":"Direct Firebolt sink is not yet supported.","title":"\ud83d\udfe1 Firebolt"},{"location":"sinks/http/","tags":["sink","http","https"],"text":"\ud83d\udfe2 HTTP/S The http/s sink writes events via batched POST requests to the configured valid and invalid urls. Without discretion. Sample HTTP/S Sink Configuration sinks: - name: somewheres type: https deliveryRequired: true validUrl: https://your-endpoint.net/valid-events-here invalidUrl: https://your-endpoint.net/invalid-events-here","title":"\ud83d\udfe2 HTTP/S"},{"location":"sinks/http/#https","text":"The http/s sink writes events via batched POST requests to the configured valid and invalid urls. Without discretion.","title":"\ud83d\udfe2 HTTP/S"},{"location":"sinks/http/#sample-https-sink-configuration","text":"sinks: - name: somewheres type: https deliveryRequired: true validUrl: https://your-endpoint.net/valid-events-here invalidUrl: https://your-endpoint.net/invalid-events-here","title":"Sample HTTP/S Sink Configuration"},{"location":"sinks/indicative/","tags":["sink","saas","indicative"],"text":"\ud83d\udfe1 Indicative Direct Indicative sink is not yet supported.","title":"\ud83d\udfe1 Indicative"},{"location":"sinks/indicative/#indicative","text":"Direct Indicative sink is not yet supported.","title":"\ud83d\udfe1 Indicative"},{"location":"sinks/kafka/","tags":["sink","stream","redpanda","kafka"],"text":"\ud83d\udfe2 Redpanda/Kafka The Redpanda/Kafka sink writes valid and invalid events to the respective topics. Sample Redpanda/Kafka Sink Configuration sinks: - name: \u5927\u718a\u732b type: redpanda deliveryRequired: true kafkaBrokers: - 127.0.0.1:9092 validTopic: honeypot-valid invalidTopic: honeypot-invalid","title":"\ud83d\udfe2 Redpanda/Kafka"},{"location":"sinks/kafka/#redpandakafka","text":"The Redpanda/Kafka sink writes valid and invalid events to the respective topics.","title":"\ud83d\udfe2 Redpanda/Kafka"},{"location":"sinks/kafka/#sample-redpandakafka-sink-configuration","text":"sinks: - name: \u5927\u718a\u732b type: redpanda deliveryRequired: true kafkaBrokers: - 127.0.0.1:9092 validTopic: honeypot-valid invalidTopic: honeypot-invalid","title":"Sample Redpanda/Kafka Sink Configuration"},{"location":"sinks/kinesis-firehose/","tags":["sink","stream","aws","kinesis firehose"],"text":"\ud83d\udfe2 AWS Kinesis Firehose The Kinesis Firehose sink writes valid and invalid events to the configured streams. It is especially useful when wanting to write incoming events directly to S3. Sample AWS Kinesis Firehose Sink Configuration sinks: - name: straightshots3 type: kinesis-firehose deliveryRequired: true validStream: honeypot-valid invalidStream: honeypot-invalid","title":"\ud83d\udfe2 AWS Kinesis Firehose"},{"location":"sinks/kinesis-firehose/#aws-kinesis-firehose","text":"The Kinesis Firehose sink writes valid and invalid events to the configured streams. It is especially useful when wanting to write incoming events directly to S3.","title":"\ud83d\udfe2 AWS Kinesis Firehose"},{"location":"sinks/kinesis-firehose/#sample-aws-kinesis-firehose-sink-configuration","text":"sinks: - name: straightshots3 type: kinesis-firehose deliveryRequired: true validStream: honeypot-valid invalidStream: honeypot-invalid","title":"Sample AWS Kinesis Firehose Sink Configuration"},{"location":"sinks/kinesis/","tags":["sink","stream","aws","kinesis"],"text":"\ud83d\udfe2 AWS Kinesis The Kinesis sink writes valid and invalid events to the configured streams. Sample AWS Kinesis Sink Configuration sinks: - name: zoom type: kinesis deliveryRequired: true validStream: honeypot-valid invalidStream: honeypot-invalid","title":"\ud83d\udfe2 AWS Kinesis"},{"location":"sinks/kinesis/#aws-kinesis","text":"The Kinesis sink writes valid and invalid events to the configured streams.","title":"\ud83d\udfe2 AWS Kinesis"},{"location":"sinks/kinesis/#sample-aws-kinesis-sink-configuration","text":"sinks: - name: zoom type: kinesis deliveryRequired: true validStream: honeypot-valid invalidStream: honeypot-invalid","title":"Sample AWS Kinesis Sink Configuration"},{"location":"sinks/logdna/","tags":["sink","saas","logdna"],"text":"\ud83d\udfe1 Logdna Direct Logdna sink is not yet supported.","title":"\ud83d\udfe1 Logdna"},{"location":"sinks/logdna/#logdna","text":"Direct Logdna sink is not yet supported.","title":"\ud83d\udfe1 Logdna"},{"location":"sinks/materialize/","tags":["sink","db","materialize"],"text":"\ud83d\udfe2 Materialize The Materialize sink writes valid and invalid events to the configured tables. This sink is especially useful when wanting to try out a streaming database without the overhead of another set of infrastructure. Destination tables are ensured on Honeypot startup, so manual creation is not required. Sample Materialize Sink Configuration sinks: - name: \ud83d\ude80\ud83d\ude80\ud83d\ude80 type: materialize deliveryRequired: true mzHost: 127.0.0.1 mzPort: 6875 mzDbName: materialize mzUser: materialize mzPass: \"\" validTable: honeypot_valid invalidTable: honeypot_invalid","title":"\ud83d\udfe2 Materialize"},{"location":"sinks/materialize/#materialize","text":"The Materialize sink writes valid and invalid events to the configured tables. This sink is especially useful when wanting to try out a streaming database without the overhead of another set of infrastructure. Destination tables are ensured on Honeypot startup, so manual creation is not required.","title":"\ud83d\udfe2 Materialize"},{"location":"sinks/materialize/#sample-materialize-sink-configuration","text":"sinks: - name: \ud83d\ude80\ud83d\ude80\ud83d\ude80 type: materialize deliveryRequired: true mzHost: 127.0.0.1 mzPort: 6875 mzDbName: materialize mzUser: materialize mzPass: \"\" validTable: honeypot_valid invalidTable: honeypot_invalid","title":"Sample Materialize Sink Configuration"},{"location":"sinks/mongodb/","tags":["sink","db","mongo","mongodb"],"text":"\ud83d\udfe2 Mongodb The Mongodb sink writes valid and invalid events to the configured collections. Collections are ensured via the nature of Mongodb, so manual creation is not required. Sample Mongodb Sink Configuration sinks: - name: docsfordays type: mongodb deliveryRequired: true mongoHosts: - mongodb1 - mongodb2 mongoPort: 27017 mongoDbName: honeypot mongoUser: hpt mongoPass: hpt validCollection: honeypotValid invalidCollection: honeypotInvalid","title":"\ud83d\udfe2 Mongodb"},{"location":"sinks/mongodb/#mongodb","text":"The Mongodb sink writes valid and invalid events to the configured collections. Collections are ensured via the nature of Mongodb, so manual creation is not required.","title":"\ud83d\udfe2 Mongodb"},{"location":"sinks/mongodb/#sample-mongodb-sink-configuration","text":"sinks: - name: docsfordays type: mongodb deliveryRequired: true mongoHosts: - mongodb1 - mongodb2 mongoPort: 27017 mongoDbName: honeypot mongoUser: hpt mongoPass: hpt validCollection: honeypotValid invalidCollection: honeypotInvalid","title":"Sample Mongodb Sink Configuration"},{"location":"sinks/mysql/","tags":["sink","db","mysql"],"text":"\ud83d\udfe2 MySQL The MySQL sink writes valid and invalid events to the configured tables. It is especially useful if you already have MySQL running and want to quickly get started with Honeypot-based event tracking. Tables are ensured upon Honeypot startup, so manual creation is not required. Sample MySQL Sink Configuration sinks: - name: whoa-nelly type: mysql deliveryRequired: true mysqlHost: localhost mysqlDbName: honeypot mysqlPort: 3306 mysqlUser: honeypot mysqlPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"\ud83d\udfe2 MySQL"},{"location":"sinks/mysql/#mysql","text":"The MySQL sink writes valid and invalid events to the configured tables. It is especially useful if you already have MySQL running and want to quickly get started with Honeypot-based event tracking. Tables are ensured upon Honeypot startup, so manual creation is not required.","title":"\ud83d\udfe2 MySQL"},{"location":"sinks/mysql/#sample-mysql-sink-configuration","text":"sinks: - name: whoa-nelly type: mysql deliveryRequired: true mysqlHost: localhost mysqlDbName: honeypot mysqlPort: 3306 mysqlUser: honeypot mysqlPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"Sample MySQL Sink Configuration"},{"location":"sinks/nats/","tags":["sink","message broker","nats"],"text":"\ud83d\udfe2\ud83c\udf89 NATS The NATS sink writes valid and invalid events to the configured subjects. It is especially useful if you already have NATS running and want to quickly get started with Honeypot-based event tracking. Sample NATS Sink Configuration sinks: - name: streamjet type: nats deliveryRequired: true natsHost: nats natsUser: someuser natsPass: somepass validSubject: honeypot.valid invalidSubject: honeypot.invalid","title":"\ud83d\udfe2\ud83c\udf89 NATS"},{"location":"sinks/nats/#nats","text":"The NATS sink writes valid and invalid events to the configured subjects. It is especially useful if you already have NATS running and want to quickly get started with Honeypot-based event tracking.","title":"\ud83d\udfe2\ud83c\udf89 NATS"},{"location":"sinks/nats/#sample-nats-sink-configuration","text":"sinks: - name: streamjet type: nats deliveryRequired: true natsHost: nats natsUser: someuser natsPass: somepass validSubject: honeypot.valid invalidSubject: honeypot.invalid","title":"Sample NATS Sink Configuration"},{"location":"sinks/natsJetstream/","tags":["sink","message broker","nats"],"text":"\ud83d\udfe1 NATS Jetstream The NATS Jetstream sink is not yet supported.","title":"\ud83d\udfe1 NATS Jetstream"},{"location":"sinks/natsJetstream/#nats-jetstream","text":"The NATS Jetstream sink is not yet supported.","title":"\ud83d\udfe1 NATS Jetstream"},{"location":"sinks/postgres/","tags":["sink","db","postgres"],"text":"\ud83d\udfe2 Postgres The Postgres sink writes valid and invalid events to the configured Postgres tables. It is especially useful if you already have Postgres running and want to quickly get started with Honeypot-based event tracking. Tables are ensured upon Honeypot startup, so manual creation is not required. Sample Postgres Sink Configuration sinks: - name: ol-trusty type: postgres deliveryRequired: true pgHost: localhost pgPort: 5432 pgDbName: honeypot pgUser: honeypot pgPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"\ud83d\udfe2 Postgres"},{"location":"sinks/postgres/#postgres","text":"The Postgres sink writes valid and invalid events to the configured Postgres tables. It is especially useful if you already have Postgres running and want to quickly get started with Honeypot-based event tracking. Tables are ensured upon Honeypot startup, so manual creation is not required.","title":"\ud83d\udfe2 Postgres"},{"location":"sinks/postgres/#sample-postgres-sink-configuration","text":"sinks: - name: ol-trusty type: postgres deliveryRequired: true pgHost: localhost pgPort: 5432 pgDbName: honeypot pgUser: honeypot pgPass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"Sample Postgres Sink Configuration"},{"location":"sinks/pubnub/","tags":["sink","pubnub"],"text":"\ud83d\udfe2 PubNub The PubNub sink writes incoming events to... PubNub ! Sample PubNub Sink Configuration sinks: - name: someapp type: pubnub deliveryRequired: true pubnubPubKey: YOUR-PUB-KEY pubnubSubKey: YOUR-SUB-KEY validChannel: honeypot-valid invalidChannel: honeypot-invalid","title":"\ud83d\udfe2 PubNub"},{"location":"sinks/pubnub/#pubnub","text":"The PubNub sink writes incoming events to... PubNub !","title":"\ud83d\udfe2 PubNub"},{"location":"sinks/pubnub/#sample-pubnub-sink-configuration","text":"sinks: - name: someapp type: pubnub deliveryRequired: true pubnubPubKey: YOUR-PUB-KEY pubnubSubKey: YOUR-SUB-KEY validChannel: honeypot-valid invalidChannel: honeypot-invalid","title":"Sample PubNub Sink Configuration"},{"location":"sinks/pubsub/","tags":["sink","gcp","pubsub"],"text":"\ud83d\udfe2 Google Pub/Sub The Google Pub/Sub sink writes valid and invalid events to the configured topics. Sample Google Pub/Sub Sink Configuration sinks: - name: googd type: pubsub deliveryRequired: true project: silverton validTopic: honeypot-valid invalidTopic: honeypot-invalid","title":"\ud83d\udfe2 Google Pub/Sub"},{"location":"sinks/pubsub/#google-pubsub","text":"The Google Pub/Sub sink writes valid and invalid events to the configured topics.","title":"\ud83d\udfe2 Google Pub/Sub"},{"location":"sinks/pubsub/#sample-google-pubsub-sink-configuration","text":"sinks: - name: googd type: pubsub deliveryRequired: true project: silverton validTopic: honeypot-valid invalidTopic: honeypot-invalid","title":"Sample Google Pub/Sub Sink Configuration"},{"location":"sinks/pulsar/","tags":["sink","message broker","stream","pulsar"],"text":"\ud83d\udfe1 Pulsar The Pulsar sink is not yet supported.","title":"\ud83d\udfe1 Pulsar"},{"location":"sinks/pulsar/#pulsar","text":"The Pulsar sink is not yet supported.","title":"\ud83d\udfe1 Pulsar"},{"location":"sinks/rabbitmq/","tags":["sink","message broker","rabbitmq"],"text":"\ud83d\udfe1 RabbitMQ The RabbitMQ sink is not yet supported \ud83d\ude31.","title":"\ud83d\udfe1 RabbitMQ"},{"location":"sinks/rabbitmq/#rabbitmq","text":"The RabbitMQ sink is not yet supported \ud83d\ude31.","title":"\ud83d\udfe1 RabbitMQ"},{"location":"sinks/redshift/","tags":["sink","dw","redshift"],"text":"\ud83d\udfe1 Redshift Direct Redshift sink is not yet supported.","title":"\ud83d\udfe1 Redshift"},{"location":"sinks/redshift/#redshift","text":"Direct Redshift sink is not yet supported.","title":"\ud83d\udfe1 Redshift"},{"location":"sinks/relay/","tags":["sink","honeypot","relay"],"text":"\ud83d\udfe2 Honeypot Relay The Honeypot Relay sink writes events to another Honeypot with the relay input enabled. Is is primarily useful when you have to get events out of dodge . Sample Honeypot Relay Sink Configuration sinks: - name: pot-o-honey type: relay deliveryRequired: true relayUrl: https://another-honeypot.net/relay","title":"\ud83d\udfe2 Honeypot Relay"},{"location":"sinks/relay/#honeypot-relay","text":"The Honeypot Relay sink writes events to another Honeypot with the relay input enabled. Is is primarily useful when you have to get events out of dodge .","title":"\ud83d\udfe2 Honeypot Relay"},{"location":"sinks/relay/#sample-honeypot-relay-sink-configuration","text":"sinks: - name: pot-o-honey type: relay deliveryRequired: true relayUrl: https://another-honeypot.net/relay","title":"Sample Honeypot Relay Sink Configuration"},{"location":"sinks/snowflake/","tags":["sink","dw","snowflake","databricks"],"text":"\ud83d\udfe1 Snowflake Direct Snowflake sink is not yet supported.","title":"\ud83d\udfe1 Snowflake"},{"location":"sinks/snowflake/#snowflake","text":"Direct Snowflake sink is not yet supported.","title":"\ud83d\udfe1 Snowflake"},{"location":"sinks/stdout/","tags":["sink","stdout"],"text":"\ud83d\udfe2 Stdout The stdout sink writes colorized events to.... stdout! It is especially useful when wanting feedback during development or when taking Honeypot for a test drive. Sample Stdout Sink Configuration sinks: - name: console type: stdout deliveryRequired: true","title":"\ud83d\udfe2 Stdout"},{"location":"sinks/stdout/#stdout","text":"The stdout sink writes colorized events to.... stdout! It is especially useful when wanting feedback during development or when taking Honeypot for a test drive.","title":"\ud83d\udfe2 Stdout"},{"location":"sinks/stdout/#sample-stdout-sink-configuration","text":"sinks: - name: console type: stdout deliveryRequired: true","title":"Sample Stdout Sink Configuration"},{"location":"sinks/timescaledb/","tags":["sink","db","timescale"],"text":"\ud83d\udfe2\ud83c\udf89 Timescale The Timescale sink writes valid and invalid events to the configured tables. It is especially useful if you already have Timescale running and want to quickly get started with Honeypot-based event tracking. Tables are ensured upon Honeypot startup, so manual creation is not required. Sample Timescale Sink Configuration sinks: - name: ol-trusty type: timescale deliveryRequired: true timescaleHost: localhost timescalePort: 5432 timescaleDbName: honeypot timescaleUser: honeypot timescalePass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"\ud83d\udfe2\ud83c\udf89 Timescale"},{"location":"sinks/timescaledb/#timescale","text":"The Timescale sink writes valid and invalid events to the configured tables. It is especially useful if you already have Timescale running and want to quickly get started with Honeypot-based event tracking. Tables are ensured upon Honeypot startup, so manual creation is not required.","title":"\ud83d\udfe2\ud83c\udf89 Timescale"},{"location":"sinks/timescaledb/#sample-timescale-sink-configuration","text":"sinks: - name: ol-trusty type: timescale deliveryRequired: true timescaleHost: localhost timescalePort: 5432 timescaleDbName: honeypot timescaleUser: honeypot timescalePass: honeypot validTable: honeypot_valid invalidTable: honeypot_invalid","title":"Sample Timescale Sink Configuration"},{"location":"sinks/vitess/","tags":["sink","db","vitess"],"text":"\ud83d\udfe1 Vitess Direct Vitess sink is not yet supported.","title":"\ud83d\udfe1 Vitess"},{"location":"sinks/vitess/#vitess","text":"Direct Vitess sink is not yet supported.","title":"\ud83d\udfe1 Vitess"},{"location":"tools/squawkbox/","tags":["tools","squawkbox"],"text":"\ud83d\udfe2 Squawkbox","title":"\ud83d\udfe2 Squawkbox"},{"location":"tools/squawkbox/#squawkbox","text":"","title":"\ud83d\udfe2 Squawkbox"},{"location":"validation-protocols/jsonschema/","tags":["collector","validation protocol","jsonschema"],"text":"\ud83d\udfe2 Jsonschema Jsonschema-based Event Validation Incoming events (across the various event protocols) are validated using a jsonschema associated with the event. Supported Drafts Honeypot event validation supports Draft 7 and Draft 2019-09 . Sample Jsonschemas For samples, visit the schemas directory in the Honeypot repo. Learning Good getting-started material for jsonschemas can be found here . The Spec The full jsonschema spec can be found here .","title":"\ud83d\udfe2 Jsonschema"},{"location":"validation-protocols/jsonschema/#jsonschema","text":"","title":"\ud83d\udfe2 Jsonschema"},{"location":"validation-protocols/jsonschema/#jsonschema-based-event-validation","text":"Incoming events (across the various event protocols) are validated using a jsonschema associated with the event.","title":"Jsonschema-based Event Validation"},{"location":"validation-protocols/jsonschema/#supported-drafts","text":"Honeypot event validation supports Draft 7 and Draft 2019-09 .","title":"Supported Drafts"},{"location":"validation-protocols/jsonschema/#sample-jsonschemas","text":"For samples, visit the schemas directory in the Honeypot repo.","title":"Sample Jsonschemas"},{"location":"validation-protocols/jsonschema/#learning","text":"Good getting-started material for jsonschemas can be found here .","title":"Learning"},{"location":"validation-protocols/jsonschema/#the-spec","text":"The full jsonschema spec can be found here .","title":"The Spec"}]}